<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-17T14:21:52+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Reflections on the Role of an AI Research Engineer (Part 1)</title><link href="http://localhost:4000/blog/2023/research_engingeer_role_ENG/" rel="alternate" type="text/html" title="Reflections on the Role of an AI Research Engineer (Part 1)" /><published>2023-11-17T21:23:41+09:00</published><updated>2023-11-17T21:23:41+09:00</updated><id>http://localhost:4000/blog/2023/research_engingeer_role_ENG</id><content type="html" xml:base="http://localhost:4000/blog/2023/research_engingeer_role_ENG/"><![CDATA[<p>Reflections on the Role of an AI Research Engineer (Part 1)</p>

<p>At least until early 2022, that’s how it was. Especially considering the periods excluding those when I worked closely with the industry during my career.</p>

<p>Let’s admit it… Despite working hard, there was undoubtedly an aspect where many AI practitioners, including myself, failed to bridge the gap between academia and industry.</p>

<p>A significant portion of my weekly reports were mainly filled with discussions of experiments and analyses of papers to see if there was a “% performance improvement.” Although I brought in the latest SOTA paper models, it was common to face dissatisfaction within my industry or domain, leading me to spend considerable time analyzing the reasons behind it. Occasionally, I found myself justifying this discrepancy with thoughts like “Maybe it’s not just about using AI, but also about domain-specific knowledge” and “Perhaps this domain is too easy,” amidst sporadic news of AI commercialization.</p>

<p>Even when I worked diligently, if the results were lacking, I often found myself casually dismissing the stress with phrases like “It’s a pity, but it can’t be helped. It seems this is the current state of the art. Maybe with a little more time…” This led to many projects ending without much pressure on my performance or reputation, focusing instead on the challenge, with some even resulting in applause from colleagues or superiors. Occasionally, I deluded myself into thinking I had achieved something significant, focusing on marginal improvements. (I couldn’t help but wonder how challenging it must have been for engineers in AI startups who hadn’t experienced this gap…)</p>

<p>In reality, there were instances where I presented a single fine-tuning as a significant achievement to my superiors or within the industry, and of course, in such situations, it was difficult to systematically evaluate AI tasks. Therefore, leveraging this characteristic, I often found myself adjusting the pace of work according to my whims, especially when it came to submitting papers for academic conferences, a luxury unthinkable in other industries. Sometimes, during meet-ups with peers from similar fields, conversations were dominated by a mix of pride and anxiety. Ironically, despite having similar thought patterns myself, I found it challenging to collaborate with colleagues whose tendencies were even more pronounced, constantly striving to find the optimal balance within this environment, aligning with my practical focus on tangible outcomes.</p>

<p>Despite numerous instances where research outcomes from academia failed to meet the demands of industry (be it from operations or customers), many AI tasks were still dependent on academic standards, trapped in a dilemma where there seemed to be no alternative. I believed that this dilemma was well exemplified in joint industry-academia projects. Although we brought industry problems to the table, trying to solve them based on academic standards often resulted in leaving the issues unresolved, merely leaving behind good references, and applauding ourselves, which was a common occurrence. Of course, there might have been other positive effects (such as strategic relationship building), but…</p>

<p>However, with the emergence of Stable-diffusion (or LLM), the situation changed dramatically. It was a watershed moment when AI began to seamlessly integrate into business operations. Apart from applications in different domains, the sheer volume of SD-related attempts pouring in, especially compared to the difficulty of reproducing performances from academic papers, was staggering. Essentially, SD shattered the cartel-like dominance of AI experimentation, which was previously exclusive to researchers, making high-quality image generation and editing accessible to everyone. The reverence once blindly directed towards YouTube channels like Two Minutes Paper or AK tweets discussing papers significantly decreased. Instead, examples receiving numerous upvotes on Reddit garnered attention, and during the era of GANs, even non-specialists actively contributed to both domestic and international communities, resulting in a multitude of outcomes. Consequently, newcomers to the field often turned to Reddit and social media before consulting survey papers.</p>

<p>As the community witnessed an influx of reproducible attempts through collective intelligence, bridging the gap between academia and industry, many organizations began minimizing their reliance on academia to create AI solutions. Some astute organizations quickly seized upon this trend, riding on the big tech model without investing in academia-style research resources, thereby achieving visible results. However, for others, it became a period of confusion, as they attempted to reconcile the strengths of existing business processes with new changes.</p>

<p>One thing is clear: AI engineers are now facing unprecedented demands for commercialization. Gone are the days of merely applauding surface-level achievements or packaging modest results in beautifully crafted reports or AI hype. As AI becomes integrated into the realm of industry, the perception of it as a highly innovative precursor task is evolving, demanding rigorous performance and evaluation standards akin to those in the industry’s mainstream.</p>

<p>(Sorce: <a href="https://www.linkedin.com/in/mark-kim-18431346/recent-activity/all/">Linkedin post</a>)</p>]]></content><author><name></name></author><category term="AI" /><category term="AI" /><category term="StableDiffusion" /><category term="ENG" /><summary type="html"><![CDATA[Reflections on the Role of an AI Research Engineer (Part 1)]]></summary></entry><entry><title type="html">The difference between DS and AI tasks</title><link href="http://localhost:4000/blog/2023/ds_ai_ENG/" rel="alternate" type="text/html" title="The difference between DS and AI tasks" /><published>2023-03-25T00:40:16+09:00</published><updated>2023-03-25T00:40:16+09:00</updated><id>http://localhost:4000/blog/2023/ds_ai_ENG</id><content type="html" xml:base="http://localhost:4000/blog/2023/ds_ai_ENG/"><![CDATA[<p>Over the past few years, through the tumultuous journey of data science (DS) and deep learning (DL) modeling, an interesting observation has emerged. It pertains to how people perceive “models (algorithms)” when faced with problems.</p>

<p>Firstly, individuals who have accumulated experience/degrees in DS often view models as tools. It’s akin to a driver choosing the appropriate car (model) for a given situation. Just as a driver doesn’t need to be an automotive engineer to reach from point A to B in the shortest time, they focus on honing their driving skills with the chosen car. This group tends to rely heavily on their past successful experiences (“I’ve driven like this before!”) because many problems they encounter are influenced more by factors other than the model’s performance (e.g., data quality, stakeholder management). Those who excel in this group skillfully blend their experiences and tools to craft compelling analytical narratives that captivate stakeholders, while the opposite case often yields uninspiring stories due to either a shallow understanding of tools or delving too deep into them.</p>

<p>However, those in the DL field exhibit a different tendency. They predominantly view models not as means but as ends in themselves. Comparatively, it’s like valuing the enhancement of a car’s performance over being recognized as a skilled driver. Hence, even if one isn’t a proficient driver, they believe improving the car’s performance is (preferably) essential to reach from point A to B quickly. This group often relies heavily on past research records (“In this paper, we did it like this!”) and shows a keen interest in research results/publishing, as there are instances where tasks that were once challenging suddenly become solvable with state-of-the-art (SOTA) papers. Those who excel in this group seamlessly integrate model objectives with business goals, creating enjoyable situations for themselves and their companies. Conversely, some fall into the trap of believing in the omnipotence of models, focusing solely on studying models and blaming external factors (data, management decisions, etc.) when things don’t go as planned.</p>

<p>Based on this, let’s summarize:</p>

<p>Unless one falls into the extreme spectrums of DS (e.g., BI) and DL (research-focused organizations), it’s advisable to frequently ask oneself such questions: “Am I viewing models as tools or as ends in themselves for the work I’m currently doing?” Without clear guidance on this, it’s surprisingly common to end up doing work for the sake of it. Looking back, I recall several projects where I confused this, digging myself into deep holes… If finding clarity is difficult, it’s also good to ask colleagues or supervisors. I should ask again tomorrow…</p>

<p>It’s desirable to extract the strengths of both groups and become someone who can digest them into my own for various problem situations (business jobs, research jobs, etc.). Currently, I feel fortunate to be influenced by excellent colleagues from both groups. Although, sometimes, being influenced by both sides might put me in a somewhat ambiguous position… But well, I believe that by continuing to enjoy and strive, it’ll be a positive stimulus for growth.
(Sorce: <a href="https://www.linkedin.com/in/mark-kim-18431346/recent-activity/all/">Linkedin post</a>)</p>]]></content><author><name></name></author><category term="AI" /><category term="AI" /><category term="DS" /><category term="ENG" /><summary type="html"><![CDATA[Over the past few years, through the tumultuous journey of data science (DS) and deep learning (DL) modeling, an interesting observation has emerged. It pertains to how people perceive “models (algorithms)” when faced with problems.]]></summary></entry><entry><title type="html">Why I’m Interested in MLOps (Part 2)</title><link href="http://localhost:4000/blog/2022/mlops_2_ENG/" rel="alternate" type="text/html" title="Why I’m Interested in MLOps (Part 2)" /><published>2022-03-04T21:40:52+09:00</published><updated>2022-03-04T21:40:52+09:00</updated><id>http://localhost:4000/blog/2022/mlops_2_ENG</id><content type="html" xml:base="http://localhost:4000/blog/2022/mlops_2_ENG/"><![CDATA[<p>Why I’m Interested in MLOps (Part 2)</p>

<p>Now, I’d like to share my perspective not as an individual but as a practitioner belonging to an organization. Ever since I first encountered MLOps, I’ve had one lingering question: “While there are plenty of successful case studies on the implementation of MLOps, why are successful utilization cases so rare?”</p>

<p>As mentioned earlier, I don’t have direct involvement in MLOps. However, through (1) consistent study of MLOps-related best practices abroad, (2) collaboration with the Google Cloud AI team, allowing me to understand GCP’s MLOps strategy, and (3) witnessing failures in the field of analytics consulting, I want to analyze this phenomenon from a slightly different perspective than traditional MLOps experts.</p>

<p>To cut to the chase, from observing various cases, it’s often difficult to find a clear answer as to “why” MLOps is ultimately being implemented. There were many instances where MLOps was pursued for seemingly “ancillary goals.” For example, because others are doing it, because GPU model resource utilization is easy, because deployment is easy, because versioning is possible, and so on. These relatively low-difficulty and comfortable ancillary goals often lead to a side effect… The fundamental goal of MLOps is set around technology-centric, fragmented topics, resulting in excellent backend experts with “limited” ML experience and capabilities often taking the lead.</p>

<p>In reality, an excellent MLOps implementation lead requires skills surpassing those of a unicorn full-stack developer when viewed from a development perspective. They must have a solid foundation in backend development, a rich understanding and experience in ML, and insights into the relevant business domain. Only with such capabilities can they select and implement frameworks that align with the fundamental “why” question, and continuously adjust the framework to accommodate the ever-changing business landscape and evolving capabilities of team members. However, finding such advanced talent is incredibly challenging, and with the increasing prevalence of MLOps, it’s common for groups rich in backend experience to take the lead, with related ML practitioners passively offering their opinions.</p>

<p>Unfortunately, what often gets overlooked in this direction is that ultimately, MLOps revolves around the user. For example, when creating a mobile app for food delivery, much investment is made in UI/UX to maximize customer satisfaction and ultimately increase revenue. However, surprisingly, MLOps often fails to consider the convenience and satisfaction of the users during implementation. In other words, the implementing organization leverages its backend expertise, gathers feedback from users to build a system well, but due to subtle differences in direction and goals, users may experience limitations in usage, unable to provide feedback easily.</p>

<p>In fact, this is not just my personal observation. It’s an issue that permeates through the book “Effective Data Infrastructure,” which discusses Metaflow (an open-source MLOps tool developed by Netflix). While reading the book, I was struck by how vehemently it advocates a philosophy centered around the user (the individuals creating ML/DL models). It’s designed to offer MLOps comprehensively to users with various levels of CS knowledge across different departments (including marketing, HR, and AI research) and aims to transform the company’s core business itself into an ML-driven one through MLOps. Of course, this is just one example of a specific framework, and naturally, other companies/organizations should establish their own intrinsic goals and select the optimal framework accordingly.</p>

<p>To summarize… 1) MLOps practitioners are exceptional individuals, and 2) organizations should select frameworks that align with clear objectives. While there are currently numerous frameworks in competition, akin to how Caffe with its awkward interface eventually faded into obscurity due to the limitations of its static graph structure, I believe various MLOps frameworks will also fall by the wayside in the user-centric competition. Regarding 2), if the organization has the resources, they can establish a unicorn team capable of deriving goals and building a collective intelligence system, or if talent acquisition is challenging, they can actively leverage external vendor expert resources (such as AWS or GCP SA). Especially with the former, while it’s a collective intelligence system, it requires backend personnel with a strong interest in ML and a proven track record of solving various ML modeling problems on their own without external guidance. Of course, such talent is naturally scarce in the market, so organizations must focus on nurturing these talents or hiring them with generous compensation if MLOps is deemed essential to their goals. (Jealous of those folks!)</p>

<p>(Sorce: <a href="https://www.linkedin.com/in/mark-kim-18431346/recent-activity/all/">Linkedin post</a>)</p>]]></content><author><name></name></author><category term="MLOps" /><category term="AI" /><category term="MLOps" /><category term="ENG" /><summary type="html"><![CDATA[Why I’m Interested in MLOps (Part 2)]]></summary></entry><entry><title type="html">Why I’m Interested in MLOps (Part 1)</title><link href="http://localhost:4000/blog/2022/mlops_1-ENG/" rel="alternate" type="text/html" title="Why I’m Interested in MLOps (Part 1)" /><published>2022-02-02T00:40:16+09:00</published><updated>2022-02-02T00:40:16+09:00</updated><id>http://localhost:4000/blog/2022/mlops_1%20ENG</id><content type="html" xml:base="http://localhost:4000/blog/2022/mlops_1-ENG/"><![CDATA[<p>Why I’m Interested in MLOps (Part 1)</p>

<p>Since last year, I’ve become increasingly fascinated by MLOps. The backstory behind this is quite intriguing… It all goes back to my time working as a Data Scientist (DS) at a previous company, where I primarily focused on building ML models for several years.</p>

<p>Back then, the term “MLOps” didn’t exist at all, and individuals with titles like Data Engineers (DE) were mainly responsible for what is now considered MLOps tasks (building data pipelines + deployment). The original responsibilities of DEs primarily revolved around building overall data pipelines, such as ETL and data mart construction, using a skill set primarily based on PL/SQL. Meanwhile, the tasks of DSs mainly involved extracting data and creating statistics or ML models based on business logic (it’s worth noting that during that time, DSs were often praised for merely creating models, ultimately leading many down the path of misplaced confidence during the flashy era of instant recognition).</p>

<p>However, a question arises here… Who deployed the models we created? At that time, deployment wasn’t given much significance, so tasks were often divided somewhat arbitrarily. For instance, DEs would write deployment-related scripts, while DSs would determine deployment cycles and strategies, implicitly delineating each other’s areas of responsibility (eventually blurring the lines between DEs and DSs). During a project at a telecommunications company, for example, we built a massive Hadoop cluster and provided in-house deployment infrastructure, allowing us to deploy models directly on top of it. Companies with resources would develop (fragmented) in-house deployment systems based on open-source tools, while others would utilize external batch tools. Back then, the focus was more on the “performance” of models rather than deployment. Thus, this arrangement was possible. In other words, the emphasis was more on the accuracy of the models, which was the central focus of C-level reports and a key determinant of project success. For instance, if a DS created a model with high accuracy based on certain insights, everyone would be pleased, and deployment would be seen as something that naturally followed.</p>

<p>However, this era where deployment was considered “natural” began to change… Particularly over the past 2-3 years, I’ve observed an interesting trend where individuals, even those without a graduate degree in ML/DL, often produce excellent models, showcasing their quick wit and prowess. This indicates that the methods for solving real-world problems using ML/DL models have already been largely standardized, and it has become increasingly apparent that applying top-tier research papers from academic conferences to real-world scenarios is challenging. Therefore, the ability to adapt existing models to practical applications has become a more pressing concern, even surpassing concerns about accuracy. As a result, ML/DL is no longer a field that primarily demands research degrees and experience; it has become a field that individuals with excellent analytical skills can also excel in (especially evident in the backgrounds of Kaggle grandmasters, many of whom are practitioners with diverse data experiences but without research degrees).</p>

<p>In this regard, as the barrier to model creation decreases and the focus shifts towards utilizing various standardized libraries and techniques to solve more business problems for more customers more efficiently, I believe we have entered a phase where we need to consider how to realize this in practice. This, I believe, is why MLOps is gaining prominence nowadays. The demand for frameworks that facilitate this realization can be seen in the proliferation of MLOps startups/frameworks, as well as in the annual influx of new services from cloud providers like AWS and the expansion of production capabilities in platforms like PyTorch’s high-level APIs and Huggingface’s introduction of spaces and acquisition of Gradio.</p>

<p>This trend also leads to a “polarization” in terms of building ML/DL models, where (1) the emphasis is on MLOps + model libraries + domain knowledge, mainly dominated by large enterprise DS/ML teams focusing on efficiency, and (2) the emphasis is on building up research expertise, mainly seen in specialized research organizations or many startups striving to raise the entry barrier. Personally, I believe that amidst this trend, “ambiguous ML/DL modelers” will inevitably fade away… (a somewhat pessimistic view perhaps); thus, I am actively studying to keep up with the latest MLOps techniques, keeping this in mind.</p>

<p>In fact, what’s a bit regrettable for me is that, as I’ve currently shifted away from DS to research and development in Computer Vision, it’s challenging for me to experience the exciting MLOps technologies pouring in recently in a practical setting. To overcome this, I’m working on toy projects using frameworks like Metaflow and actively following trends through various blogs and YouTube channels like Chip yuen’s. I’m aware that such practical opportunities will surely come back in my career, so I’m preparing myself by understanding the overall trends mentioned above and efficiently studying the latest MLOps techniques for that time.</p>

<p>(Sorce: <a href="https://www.linkedin.com/in/mark-kim-18431346/recent-activity/all/">Linkedin post</a>)</p>]]></content><author><name></name></author><category term="MLOps" /><category term="AI" /><category term="MLOps" /><category term="ENG" /><summary type="html"><![CDATA[Why I’m Interested in MLOps (Part 1)]]></summary></entry></feed>